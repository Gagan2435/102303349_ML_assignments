{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT 4"
      ],
      "metadata": {
        "id": "nPX75KSAyOwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyUTBxF2wXrQ",
        "outputId": "9e8c5800-77a2-4f56-fcb1-0255d37c54cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top results (if any):\n",
            "   Learning Rate        Lambda      Cost  R2_Score\n",
            "0           0.10  0.000000e+00  0.003467  0.949357\n",
            "1           0.10  1.000000e-15  0.003467  0.949357\n",
            "2           0.10  1.000000e-10  0.003467  0.949357\n",
            "3           0.10  1.000000e-05  0.003472  0.949316\n",
            "4           0.10  1.000000e-03  0.003904  0.945652\n",
            "5           0.01  0.000000e+00  0.004807  0.926223\n",
            "6           0.01  1.000000e-15  0.004807  0.926223\n",
            "7           0.01  1.000000e-10  0.004807  0.926223\n",
            "8           0.01  1.000000e-05  0.004807  0.926223\n",
            "9           0.01  1.000000e-03  0.004828  0.926173\n",
            "\n",
            "Invalid combos (diverged or produced non-finite values):\n",
            "[]\n",
            "\n",
            "Best combo:\n",
            "Learning Rate    0.100000\n",
            "Lambda           0.000000\n",
            "Cost             0.003467\n",
            "R2_Score         0.949357\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "# ----------------------------\n",
        "# Create dataset\n",
        "# ----------------------------\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "X_base = np.random.rand(n_samples, 1)\n",
        "X = np.hstack([X_base + np.random.normal(0, 0.01, (n_samples, 1)) for _ in range(7)])\n",
        "y = 3*X[:, 0] + 2*X[:, 1] - 4*X[:, 2] + np.random.normal(0, 0.05, n_samples)\n",
        "\n",
        "# train-test split & scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# Robust Ridge GD with NaN/inf checks\n",
        "# ----------------------------\n",
        "def is_finite_array(a):\n",
        "    return np.all(np.isfinite(a))\n",
        "\n",
        "def ridge_regression_gd_safe(X, y, lr=0.01, lam=1e-3, n_iter=1000, grad_clip=None, verbose=False):\n",
        "    n_samples, n_features = X.shape\n",
        "    w = np.zeros(n_features, dtype=float)\n",
        "    b = 0.0\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        y_pred = X.dot(w) + b\n",
        "        error = y - y_pred\n",
        "\n",
        "        # compute gradients\n",
        "        dw = (-2/n_samples) * X.T.dot(error) + 2*lam*w\n",
        "        db = (-2/n_samples) * np.sum(error)\n",
        "\n",
        "        # gradient clipping (optional)\n",
        "        if grad_clip is not None:\n",
        "            dw = np.clip(dw, -grad_clip, grad_clip)\n",
        "            db = np.clip(db, -grad_clip, grad_clip)\n",
        "\n",
        "        # update\n",
        "        w = w - lr * dw\n",
        "        b = b - lr * db\n",
        "\n",
        "        # check for NaN or inf\n",
        "        if not (is_finite_array(w) and np.isfinite(b)):\n",
        "            if verbose:\n",
        "                print(f\"Stopped at iter {it}: non-finite w or b (lr={lr}, lam={lam})\")\n",
        "            return None, None, float('inf'), False  # mark invalid\n",
        "\n",
        "        # optional early check on cost (avoid overflow)\n",
        "        cost = np.mean((y - (X.dot(w) + b))**2) + lam * np.sum(w**2)\n",
        "        if not np.isfinite(cost):\n",
        "            if verbose:\n",
        "                print(f\"Stopped at iter {it}: non-finite cost (lr={lr}, lam={lam})\")\n",
        "            return None, None, float('inf'), False\n",
        "\n",
        "    # final eval\n",
        "    final_pred = X.dot(w) + b\n",
        "    final_cost = np.mean((y - final_pred)**2) + lam * np.sum(w**2)\n",
        "    return w, b, final_cost, True\n",
        "\n",
        "# ----------------------------\n",
        "# Run grid safely and collect results\n",
        "# ----------------------------\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "results = []\n",
        "invalid_combos = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w, b, cost, valid = ridge_regression_gd_safe(X_train, y_train, lr=lr, lam=lam, n_iter=2000, grad_clip=1e6, verbose=False)\n",
        "        if not valid or w is None:\n",
        "            invalid_combos.append((lr, lam))\n",
        "            continue  # skip computing r2\n",
        "\n",
        "        y_pred_test = X_test.dot(w) + b\n",
        "        if not is_finite_array(y_pred_test):\n",
        "            invalid_combos.append((lr, lam))\n",
        "            continue\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred_test)\n",
        "        results.append({'Learning Rate': lr, 'Lambda': lam, 'Cost': cost, 'R2_Score': r2})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results_sorted = df_results.sort_values(by='R2_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Top results (if any):\")\n",
        "print(df_results_sorted.head(10))\n",
        "print(\"\\nInvalid combos (diverged or produced non-finite values):\")\n",
        "print(invalid_combos)\n",
        "\n",
        "if not df_results.empty:\n",
        "    best = df_results_sorted.iloc[0]\n",
        "    print(\"\\nBest combo:\")\n",
        "    print(best)\n",
        "else:\n",
        "    print(\"\\nNo valid combos found â€” try smaller learning rates and/or fewer iterations.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "csv_url = \"https://git.wur.nl/ridde020/msc-course-machine-learning/-/raw/main/islr_data/Hitters.csv\"\n",
        "df = pd.read_csv(csv_url, index_col=0)\n",
        "df = df.dropna(subset=[\"Salary\"])\n",
        "\n",
        "cat_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
        "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "X = df.drop(columns=[\"Salary\"])\n",
        "y = df[\"Salary\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    \"Linear\": LinearRegression(),\n",
        "    \"RidgeCV\": RidgeCV(alphas=np.logspace(-3, 3, 50), store_cv_values=True),\n",
        "    \"LassoCV\": LassoCV(alphas=None, cv=5, max_iter=5000, random_state=42),\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))   # fixed line\n",
        "    alpha = getattr(model, \"alpha_\", None)\n",
        "    results.append({\"Model\": name, \"R2\": r2, \"RMSE\": rmse, \"Alpha\": alpha})\n",
        "\n",
        "res_df = pd.DataFrame(results).sort_values(by=\"R2\", ascending=False).reset_index(drop=True)\n",
        "print(res_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB4ggLiLxO_9",
        "outputId": "72ef9bc1-d33e-4348-ed90-60dc35b1e2a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Model        R2        RMSE       Alpha\n",
            "0  RandomForest  0.459586  312.643245         NaN\n",
            "1        Linear  0.235397  371.880773         NaN\n",
            "2       LassoCV  0.209663  378.087091   25.391196\n",
            "3       RidgeCV  0.199792  380.440909  104.811313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "alphas = np.logspace(-4, 4, 50)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n",
        "lasso_cv = LassoCV(alphas=None, cv=5, max_iter=5000, random_state=42)\n",
        "\n",
        "ridge_cv.fit(X_train_s, y_train)\n",
        "lasso_cv.fit(X_train_s, y_train)\n",
        "\n",
        "y_pred_ridge = ridge_cv.predict(X_test_s)\n",
        "y_pred_lasso = lasso_cv.predict(X_test_s)\n",
        "\n",
        "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
        "lasso_r2 = r2_score(y_test, y_pred_lasso)\n",
        "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "\n",
        "print(f\"RidgeCV alpha: {ridge_cv.alpha_:.6g}, R2: {ridge_r2:.4f}, RMSE: {ridge_rmse:.4f}\")\n",
        "print(f\"LassoCV alpha: {lasso_cv.alpha_:.6g}, R2: {lasso_r2:.4f}, RMSE: {lasso_rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-JjuAs3xqxQ",
        "outputId": "9043bc0d-0fe8-4ee8-ba42-6e0d77590b91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RidgeCV alpha: 1.75751, R2: 0.5758, RMSE: 0.7455\n",
            "LassoCV alpha: 0.00079852, R2: 0.5766, RMSE: 0.7448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from scipy.special import expit  # sigmoid function\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Sigmoid (already using expit)\n",
        "def sigmoid(z):\n",
        "    return expit(z)\n",
        "\n",
        "# Cost function\n",
        "def compute_cost(y, y_hat):\n",
        "    m = len(y)\n",
        "    return -(1/m) * np.sum(y*np.log(y_hat + 1e-15) + (1-y)*np.log(1-y_hat + 1e-15))\n",
        "\n",
        "# Binary logistic regression (GD)\n",
        "def train_binary_logistic(X, y, lr=0.1, n_iter=1000, lam=0.0):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    b = 0.0\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_hat = sigmoid(z)\n",
        "        dw = (1/m) * np.dot(X.T, (y_hat - y)) + lam*w\n",
        "        db = (1/m) * np.sum(y_hat - y)\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "    return w, b\n",
        "\n",
        "# One-vs-Rest implementation\n",
        "def train_ovr(X, y, num_classes, lr=0.1, n_iter=1000):\n",
        "    models = []\n",
        "    for c in range(num_classes):\n",
        "        y_binary = np.where(y == c, 1, 0)\n",
        "        w, b = train_binary_logistic(X, y_binary, lr, n_iter)\n",
        "        models.append((w, b))\n",
        "    return models\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    probs = []\n",
        "    for w, b in models:\n",
        "        probs.append(sigmoid(np.dot(X, w) + b))\n",
        "    probs = np.array(probs).T\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "# Train models for each class (3 classes)\n",
        "models = train_ovr(X_train, y_train, num_classes=3, lr=0.1, n_iter=2000)\n",
        "\n",
        "# Predictions\n",
        "y_pred = predict_ovr(X_test, models)\n",
        "\n",
        "# Evaluation\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Compare with sklearn built-in LogisticRegression (OvR default)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_sklearn = lr_model.predict(X_test)\n",
        "print(\"Sklearn OvR Accuracy:\", accuracy_score(y_test, y_pred_sklearn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehfOXTjByGhm",
        "outputId": "b51427f5-fdec-4023-f1bf-ede0df4fbb06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "Sklearn OvR Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}